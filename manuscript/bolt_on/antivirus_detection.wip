# Detection

C> By: Thorsten Sick

C> For: Architects, Manager, Malware Analysts

%% State: 30 % Detection

Detection and filtering are bolt-on-security. It can be added to any existing system.

To avoid confusion, I will use three terms in this book and define them here:

**Detection** is using an existing database, matching that against network packets, files or behaviour on the client system and preventing
detected items from causing harm.

**Classification** is used to create such a database: Unknown objects are classified either manually or by some tools into at least the categories good/bad. Maybe we can also identify the families or the type of malware (trojan, worm).

**Analysis** Is an in depth analysis of a malware to learn about the used technology, trick and origin. An analysis is important to improve classification technology. It also produces interesting blog posts. Quite often an analysis will eat days and weeks of a specialist's time.

## Pro

* Simple to add on top of an existing system
* With bought signatures: Can filter the outer perimeter for common attacks
* Own signatures in the innermost layer identify when other security features did break
* Fast response (within minutes) - can cover the gap until a patch is rolled out

## Con

* Fast Flux beats IP filtering
* It is a statistics game. Do not expect perfect security
* Not all files are scanned (HTTPS stream: MITM or do not scan it ?)
* If file processing is not sandboxed: Not good
* If file processing is done in kernel: Double plus not good
* "Clean" detection should not be read as "Clean". It is "did not find anything"

## Tips

* Double Tap: Use two different technologies to detect one malware
* Malware behaviour does not change often. Behaviour detection can stay stable for months
* Malware hash chances constantly. Only do this if fully automated
* Detection patterns will be OK for minutes or hours
* Structural generics will be OK for hours
* Bad guys have their own "VirusTotal" malware scanner to create their malware FUD (Fully Un Detected)
** Malware authors even offer support contracts for updates should the malware be detected
** This update-test-release cycle is at least semi automatic
** Detection technology that is not accessible for the bad guys lasts a lot longer (smaller "AVs" benefit - create you own in house AV ?)
* Do not send feedback to the attack ("Your mail has been blocked"). Maybe postpone feedback for one day. That way the attacker can not tune the attack to your defense

There is a statistics trick how to boost the combination of different detection technologies.
To get a good combination combine several _different_ technologies together.

I will be using simple numbers to make a point now:
Lets assume we have three different scanners each having 90% detection (a bad detection quality).
Out of 1000 attacks 100 will pass the first scanner. 10 will pass the second scanner.
1 will get past the third one. Much better than first assumed. Very important: Make sure they use different technologies. Or the same samples
not detected by number one will also not be detected by number 2....

## Choosing technology

You can pick any AV of the top 5 in the tests. Basically you will pick the one that has

* Low resource requirements
* Stability
* Good self protection (security)
* Simple UI
* Maybe remote manageability (for companies)
* Supports your mail system, web proxy, ....
* Check for False Positives as well in those tests

It could also be important to check for privacy commitment. F-Secure has some detailed ones, for example:

https://www.f-secure.com/en/web/legal/principles

### Numbers

In tests yous should aim for a detection rating north of 95 %, a False Positive Rating south of 0.05 %.
For False Positives (detection on innocent files): If you have a well maintained
system with only basic and boring files a higher FP rating will also work for you.
If you have a heterogeneous landscape, many computers, fancy extraordinary tools:
Aim for a lower FP rating.

### Test institutions

Those institutions are the source for magazine articles.

[Virus Bulletin](https://www.virusbulletin.com/testing/vb100/)

Main challenge is stability and low FP rate here.

[Av-test](https://www.av-test.org/de/antivirus/)

Also includes usability and speed

[Av-comparatives](https://www.av-comparatives.org/dynamic-tests/)

Has monthly real-world detection tests

### Test methods

There are two major ways of testing security products:

Mass scans:

The old way. A large batch of malware is scanned. Percentage of detection is the result.
Problem here:

* Not all detection technology is tested. Especially behaviour detection is not used on static scans
* Many samples will be weeks or months old in this set. Current malware is under-represented

Real-world-tests:

A VM set prepared and a script surfs to an infected page. The whole product can now prevent infection of the system.

This has it's own challenges for the testers.

* Good: The whole product is tested
* Bad: It is hard to simulate the exact infection vector of the malware
* Bad: Testers have a small time window to test all the products against a specific malware attack (before the attack is taken down)
* Bad: Hard to reproduce
* Bad: Only small sample set because it is so complex

As for which test to trust: Go for Real-world tests. As soon as you have the
product test your specific entry points for malware with Eicar test file or
simulated malicious URLs. If the products properly cover those.

## DIY detection

There are different tools to detect malware. Normally you do have some internal
knowledge covering which apps are malicious and which benign. For more on
that see the "DIY classification" chapter. Detection is how you get this knowledge
from your lab to the computer you want to protect.

%% TODO: write DIY classification chapter

### Hashes

The simplest and most reliable way is file hashes. The good thing: If your
classification is correct they will not add FPs on their own. The bad news:
They will not detect much as malware is normally polymorphic. But if you can
create them by a script "for free" and without effort: Use them in addition
to better tools.

Important: CRC32, MD5 and SHA1 are dead.
Use Sha256 instead. If you feel fancy look at SSDeep

    Optional background: With the large number of files currently released it is
    even possible to have accidental MD5 collisions. Result will be two totally
    different files with the same MD5. And this will fuck up your DB if it is MD5 based.

### Yara

The tool to create your own file detection is Yara. A tutorial can be found here:

https://www.real0day.com/hacking-tutorials/yara

Yara combines logic and (regex) patterns into a quite simple config language.

%% TODO: Also good for binary processing is https://kaitai.io/


### SNORT

[SNORT](https://www.snort.org/) is a tool detect network packages




## DIY classification

You have an unknown sample and want to learn if it is malicious. This is the step
you normally do *before* you do detection.

**VirusTotal**: One way to do classification is to run it through a multi scanner like [VirusTotal](https://www.virustotal.com).
You will benefit from the classification work of several virus labs. Negative side effects:

* Your sample will be public
* You can never ever be the first to detect a new sample

**Cuckoo Sandbox**: An analysis sandbox. Hard to set up on your own system. But with some luck [malwr.com](https://malwr.com) is up and running. Negative side effects are:

* You will share your sample
* Sandboxes are slow (some minute per sample)
* Some malware types can not be handled by Sandboxes




%% TODO: Pareto principle
%% TODO: Statistics anomaly when combining different techs
%% TODO: Specialized and hidden detection - attacker does not get feedback
